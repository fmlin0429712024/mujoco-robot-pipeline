# Demo Design Architecture

> **Your learning project:** Complete pick-and-place IL pipeline explained

**Reading time:** 5 minutes

---

## What You Built

A complete **Imitation Learning pipeline** that teaches a 6-DOF robot arm to pick a cube and place it in a bucket.

**This is the culmination of everything you learned** - from understanding methods (doc 01) to applying them in a real project.

---

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PICK-AND-PLACE IL PIPELINE           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LAYER 1: Simulation          DATA PIPELINE         LEARNING
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MuJoCo       â”‚â”€â”€HDF5â”€â”€â”€â”€â”€â–¶â”‚ Convert  â”‚â”€â”€Parquetâ–¶â”‚ ACT        â”‚
â”‚ + Expert     â”‚  (26GB)    â”‚ to       â”‚ (447MB) â”‚ Training   â”‚
â”‚ + Random     â”‚            â”‚ LeRobot  â”‚         â”‚ 30k steps  â”‚
â”‚   cubes      â”‚            â”‚          â”‚         â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†‘                                                 â”‚
      â”‚                                                 â†“
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Trained Policy
                                                   (Evaluation)
```

---

## Architecture Layers

### **Layer 1: Simulation Core**

**Location:** `trossen_arm_mujoco/`

**What it does:** Provides physics, robot model, and task definition

**Key components:**

| File | Purpose |
|------|---------|
| `assets/trossen_one_arm_scene.xml` | Scene (arm, cube, bucket, camera) |
| `sim_env.py` | Task rewards, success detection |
| `scripted_policy.py` | Expert demonstrations (IK-based) |
| `scripts/record_sim_episodes.py` | Data collection |

**Critical design decision:**
```python
# âŒ WRONG: Identity mapping (80% of beginners make this mistake!)
action[t] = current_position  # Learns "do nothing"

# âœ… CORRECT: Next position mapping
action[t] = next_position  # qpos[t+1] - Learns movement
```

**Why this matters:** Policy must learn stateâ†’next_state transitions, NOT identity function. This single bug breaks the entire learning pipeline!

---

### **Layer 2: Data Pipeline**

**Location:** `scripts/`

**What it does:** Transforms raw data into training-ready format

**Process:**

```
Raw HDF5 (26GB)
    â†“
[scripts/create_lerobot_dataset.py]
    â†“
LeRobot Parquet (447MB)
    â”œâ”€ data.parquet (states, actions)
    â”œâ”€ videos/*.mp4 (compressed images)
    â””â”€ stats.safetensors (normalization)
```

**Compression:** 58x reduction via MP4 encoding

**Key benefit:** Faster training (column-oriented Parquet)

---

### **Layer 3: Learning**

**Location:** `scripts/train_policy.py`

**What it does:** Trains ACT policy to mimic expert

**Architecture:**

```
ACT Policy:
â”œâ”€ Vision: ResNet-18 encoder (84Ã—84 RGB â†’ features)
â”œâ”€ State: 6D joint positions
â”œâ”€ Decoder: Transformer (action chunking)
â””â”€ Output: 100-step action sequence
```

**Training configuration:**
- Steps: 30,000 (fixed, no periodic eval)
- Batch size: 8
- Learning rate: 1e-5
- Device: MPS (Apple Silicon)

**Results:**
- Initial loss: 4.374
- Final loss: 0.036
- Success rate: 0% (shows intent, needs more data)

---

## Data Flow

### **Complete Pipeline:**

```
1. Expert Demonstration
   â”œâ”€ scripted_policy.py generates perfect trajectory
   â”œâ”€ Randomized cube positions
   â””â”€ Record 50 episodes

2. Data Recording
   â”œâ”€ Save to data/raw/*.hdf5 (26GB)
   â””â”€ Images + joint positions + actions

3. Format Conversion
   â”œâ”€ HDF5 â†’ LeRobot Parquet + MP4
   â”œâ”€ Compute normalization statistics
   â””â”€ data/lerobot/ (447MB)

4. Training
   â”œâ”€ Sample batches from Parquet
   â”œâ”€ Train ACT for 30k steps
   â”œâ”€ Save checkpoints every 5k
   â””â”€ outputs/train/.../030000/

5. Evaluation
   â”œâ”€ Load trained model
   â”œâ”€ Run 10 test episodes
   â””â”€ Record video + measure success
```

---

## Critical Components

### **1. Expert Policy (Scripted)**

**File:** `scripted_policy.py`

**How it works:**
```
1. Move arm above cube
2. Lower arm
3. Close gripper (grasp)
4. Lift cube
5. Move to bucket
6. Open gripper (release)
7. Return to home
```

**Uses:** Inverse kinematics for smooth trajectories

**Why scripted?** Perfect demonstrations for IL

---

### **2. Image Preprocessing**

**Critical for success:**

```python
# Both training and eval MUST use:
image = image / 255.0  # Scale to [0,1]
mean = [0.485, 0.456, 0.406]  # ImageNet
std = [0.229, 0.224, 0.225]    # ImageNet
image = (image - mean) / std
```

**Bug we fixed:** Eval initially didn't apply ImageNet normalization â†’ policy failed

**Lesson:** Preprocessing consistency is critical!

---

### **3. Observation Space**

```python
observation = {
    'observation.state': joint_positions,  # 6D
    'observation.images.top_cam': image,   # 84Ã—84Ã—3
}
```

**Why dict format?** LeRobot standard

**Camera:** Top-down view of workspace

---

### **4. Action Space**

```python
action = next_joint_positions  # 6D
```

**NOT:** End-effector poses
**WHY:** Direct joint control is simpler for learning

---

## Training Process Explained

### **Why 30k Fixed Steps?**

**Not like this** âŒ:
```
Train 1000 â†’ Eval â†’ Check if perfect â†’ Continue
```

**Actually like this** âœ…:
```
Train 30,000 steps continuously â†’ Stop â†’ Eval once
```

**Advantages:**
- Simpler (no periodic eval logic)
- Predictable (know exactly how long it takes)
- Sufficient for this dataset size

**Timeline:**

```
Step 0        Step 15k       Step 30k
â”‚             â”‚              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Training...   Save model     STOP â†’ Eval

Loss: 4.374                  0.036
```

---

## Evaluation Design

**Process:**
1. Load trained model
2. Reset environment to random cube position
3. Run policy for max 300 steps
4. Check if cube in bucket
5. Repeat 10 times
6. Record success rate

**Current result:** 0%

**Why?** 
- Policy learned movement patterns âœ…
- Needs more data/training for precision
- Shows clear intent to pick (partial success!)

---

## Project Structure

```
trossen-pick-place/
â”‚
â”œâ”€ trossen_arm_mujoco/       # Layer 1: Simulation
â”‚  â”œâ”€ assets/                # MuJoCo scenes
â”‚  â”œâ”€ sim_env.py             # Task logic
â”‚  â”œâ”€ scripted_policy.py     # Expert
â”‚  â””â”€ scripts/
â”‚     â””â”€ record_sim_episodes.py
â”‚
â”œâ”€ scripts/                  # Layer 2 & 3
â”‚  â”œâ”€ create_lerobot_dataset.py
â”‚  â”œâ”€ train_policy.py
â”‚  â”œâ”€ eval_policy.py
â”‚  â””â”€ visualize_*.py
â”‚
â”œâ”€ data/
â”‚  â””â”€ lerobot/               # Training data (447MB)
â”‚
â”œâ”€ outputs/
â”‚  â””â”€ train/.../030000/      # Trained model (591MB)
â”‚
â””â”€ visualizations/           # Demo videos (27MB)
```

---

## Key Learnings

### **1. Imitation Learning Works**

âœ… 50 demonstrations sufficient to learn movement patterns

âœ… Loss decreased from 4.374 â†’ 0.036

âœ… Policy shows clear intent (reaches toward cube)

âš ï¸ 0% success rate (needs more data/training for precision)

---

### **2. Details Matter**

**Bugs found and fixed:**

| Bug | Impact | Fix |
|-----|--------|-----|
| Action recording | Policy learned identity | Save `qpos[t+1]` |
| Image normalization | Policy failed completely | Match ImageNet stats |
| Data directory | 95GB in `~/.trossen` | Use explicit `--data_dir` |

**Lesson:** Small implementation details can break learning entirely

---

### **3. MuJoCo is Perfect for Learning**

âœ… Fast iteration (500 FPS)

âœ… Simple setup (no GPU needed)

âœ… Good enough for IL concepts

âœ… Free (learn without budget)

**What you learned here** transfers directly to Isaac Sim when you're ready to scale

---

## Success Metrics

### **What "Success" Means**

| Metric | Target | Achieved |
|--------|--------|----------|
| Training completes | Yes | âœ… Yes |
| Loss decreases | < 0.1 | âœ… 0.036 |
| Policy shows intent | Reaches cube | âœ… Yes |
| Grasps cube | > 50% rate | âŒ 0% |
| Places in bucket | > 80% rate | âŒ 0% |

**Interpretation:** Pipeline works, needs more data/training for full task completion

---

## How to Improve

**Option 1: More Data**
- Record 100-200 episodes (vs 50)
- More diverse cube positions

**Option 2: Longer Training**
- 50k-100k steps (vs 30k)
- May overfit without more data

**Option 3: Data Augmentation**
- Image transforms (brightness, rotation)
- Helps with limited data

**Option 4: Migrate to Isaac Sim**
- Domain randomization (lighting, textures)
- Better sim-to-real transfer
- If deploying to real robot

---

## What You Achieved

**From zero to complete IL pipeline in 2-3 weeks:**

âœ… Simulation setup (MuJoCo, expert policy)

âœ… Data collection (50 episodes, domain randomization)

âœ… Format conversion (HDF5 â†’ LeRobot)

âœ… Policy training (ACT transformers)

âœ… Evaluation framework

âœ… Debugging skills (action bug, normalization)

âœ… Documentation (guides, architecture)

**This knowledge is the foundation** for any robot learning project!

---

## Connecting to the Journey

**Document 01:** You learned IL is for copying experts â†’ You used it here!

**Document 02:** You saw pick-place as a use case â†’ You built it!

**Document 03:** You learned Isaac Sim for scaling â†’ Your next step!

**Document 04:** You learned MuJoCo for learning â†’ Perfect choice!

**Document 05:** You see how it all comes together in your demo âœ…

**The journey is complete.** You're now ready to tackle real robot learning challenges! ğŸš€

---

## Next Steps

**Apply your knowledge:**

1. **Improve this demo:**
   - Collect more data
   - Try longer training
   - Experiment with hyperparameters

2. **Try Isaac Sim:**
   - Migrate your pick-place task
   - Add domain randomization
   - Prepare for real robot deployment

3. **Build something new:**
   - Different manipulation task
   - Use RL instead of IL
   - Apply to your own robot project

**You have the tools. Now go build!** ğŸ¯

---

*Part 5 of 5-part learning journey - Complete!*

**â†’ Back to start:** [01 - Robot Learning Methods Overview](01_robot_learning_methods_overview.md)
