# System Architecture Overview

> **Quick Reference:** Understand how the robot learning pipeline works in 5 minutes

---

## ğŸ¯ What This System Does

Teaches a 6-DOF robot arm to **pick up a red cube and place it in a green bucket** using:
1. 50 expert demonstrations (scripted policy)
2. Vision-based imitation learning (ACT policy)
3. Simulation-only deployment

---

## ğŸ“ Architecture (4 Layers)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ROBOT LEARNING PIPELINE                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Layer 1: SIMULATION          Layer 2: DATA           Layer 3: LEARNING
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ trossen_arm_     â”‚         â”‚ scripts/     â”‚        â”‚ scripts/     â”‚
â”‚ mujoco/          â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚              â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚              â”‚
â”‚                  â”‚         â”‚              â”‚        â”‚              â”‚
â”‚ â€¢ Scene (XML)    â”‚  HDF5   â”‚ â€¢ Convert    â”‚ Parquetâ”‚ â€¢ Train      â”‚
â”‚ â€¢ Expert policy  â”‚  26GB   â”‚ â€¢ Visualize  â”‚ 447MB  â”‚ â€¢ Evaluate   â”‚
â”‚ â€¢ Record script  â”‚         â”‚              â”‚        â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ What Each Directory Does

| Directory | Purpose | Key Files |
|-----------|---------|-----------|
| **`trossen_arm_mujoco/`** | Simulation core | `sim_env.py` (task), `scripted_policy.py` (expert), `record_sim_episodes.py` (data collection) |
| **`scripts/`** | Data + Learning pipeline | `create_lerobot_dataset.py` (convert), `train_policy.py` (train), `eval_policy.py` (evaluate) |
| **`data/lerobot/`** | Training dataset | Parquet tables + MP4 videos (447MB) |
| **`outputs/train/`** | Trained models | Checkpoints (591MB) |
| **`visualizations/`** | Demo videos | Expert, random, before/after training |

---

## ğŸ”„ Complete Data Flow

```
1. Expert Demos
   â”œâ”€â–¶ record_sim_episodes.py
   â””â”€â–¶ data/raw/*.hdf5 (26GB)

2. Convert Format
   â”œâ”€â–¶ create_lerobot_dataset.py
   â””â”€â–¶ data/lerobot/ (447MB)            â† Delete data/raw after this!

3. Train Policy
   â”œâ”€â–¶ train_policy.py (30k steps)
   â””â”€â–¶ outputs/train/.../030000/

4. Evaluate
   â”œâ”€â–¶ eval_policy.py
   â””â”€â–¶ visualizations/after_training.mp4
```

---

## ğŸ“ Training-Eval Loop Explained

### How Training Works

**Simple Answer:** Train for a **fixed number of steps** (30,000), then stop and evaluate.

**Not like this** âŒ:
```
Train 1000 steps â†’ Eval â†’ Check if perfect â†’ If not, train more â†’ Repeat
```

**Actually like this** âœ…:
```
Train continuously for 30,000 steps â†’ Stop â†’ Eval once â†’ Done
```

### Why This Design?

| Design Choice | Reason |
|---------------|--------|
| **Fixed 30k steps** | Simpler, more predictable than convergence-based stopping |
| **No periodic eval** | Saves time, eval is expensive (run full episodes) |
| **Eval at end only** | You decide if you want to train longer after seeing results |

### Detailed Timeline

```
Step 0        Step 5k       Step 10k      Step 15k      Step 20k      Step 25k      Step 30k
â”‚             â”‚             â”‚             â”‚             â”‚             â”‚             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Training... â”‚ Save model  â”‚ Save model  â”‚ Save model  â”‚ Save model  â”‚ Save model  â”‚ STOP
â”‚             â”‚             â”‚             â”‚             â”‚             â”‚             â”‚
â”‚             checkpoint    checkpoint    checkpoint    checkpoint    checkpoint    â–¼
â”‚                                                                                    Eval
â”‚                                                                                    (10 episodes)
â”‚                                                                                    â”‚
Loss: 4.374                                                              0.036     Success: 0%
```

**What happens:**
1. **Training phase** (0-30k steps):
   - Read random batches from dataset
   - Compute loss (how different is policy from expert?)
   - Update model weights
   - Save checkpoint every 5k steps
   - **No evaluation during training**

2. **After training** (manual):
   - Run `eval_policy.py` on final checkpoint
   - Simulate 10 episodes, measure success rate
   - Record video to see what policy learned
   - **You decide:** Train longer? Collect more data? Ship it?

### When to Stop Training?

**Option A: Fixed Budget** (This project âœ…)
- Decide upfront: "Train for 30k steps"
- Eval once at end
- Simple, predictable

**Option B: Convergence-Based** (Not used here)
- Monitor validation loss during training
- Stop when loss plateaus
- More complex, needs validation set

**Why we chose Option A:**
- Simpler for learning project
- 30k steps = ~2-3 hours on Apple Silicon
- Can always train longer if needed

---

## ğŸ”§ Critical Components Explained

### 1. Action Recording (Most Important!)

**The Bug We Fixed:**
```python
# âŒ WRONG (identity mapping)
action[t] = current_position

# âœ… CORRECT (next position)
action[t] = next_position
```

**Why this matters:** Policy needs to learn "given current state, what's the next state?" not "output what you see."

### 2. Image Preprocessing

**Training and eval MUST use the same normalization:**
```python
# Both must apply ImageNet normalization
image = (image / 255.0 - mean) / std
```

**Why this matters:** Policy trained on normalized images will fail if eval gives unnormalized images.

### 3. Data Format Conversion

**HDF5 â†’ LeRobot Parquet:**
- **Why convert?** Parquet is faster for training (column-oriented)
- **Compression:** 26GB â†’ 447MB (58x smaller!)
- **Videos:** MP4 encoding instead of raw frames

---

## ğŸ’¡ Quick Reference: Common Tasks

### Task: Retrain from scratch
```bash
# Keep dataset, delete models
rm -rf outputs/train/*
python scripts/train_policy.py
```

### Task: Train longer (extend 30k â†’ 50k)
```bash
# Modify train_policy.py: steps = 50000
python scripts/train_policy.py
```

### Task: Test different checkpoint
```bash
python scripts/eval_policy.py --ckpt outputs/train/.../020000/pretrained_model
```

### Task: Free up disk space
```bash
# After conversion, delete raw data
rm -rf data/raw/*

# After training, delete intermediate checkpoints
rm -rf outputs/train/.../checkpoints/{005000,010000,015000,020000,025000}
```

---

## ğŸ¯ Success Criteria

### What "Success" Looks Like

| Metric | Target | This Project |
|--------|--------|--------------|
| Training loss | < 0.05 | âœ… 0.036 |
| Eval success rate | > 80% | âŒ 0% |
| Arm reaches cube | Yes | âœ… Yes (shows intent) |
| Arm grasps cube | Yes | âŒ Not yet |

**Interpretation:** Policy learned movement toward cube but needs more training/data for precision.

---

## ğŸš€ Next Steps to Improve

**If you want better success rate:**
1. Record more data (100+ episodes instead of 50)
2. Train longer (50k steps instead of 30k)
3. Add data augmentation (vary lighting/colors)

**If you want real robot deployment:**
1. Migrate to Isaac Sim (photorealistic)
2. Add domain randomization
3. Acquire physical Trossen arm
4. See [MuJoCo vs Isaac Sim comparison](mujoco_vs_isaac_sim.md)

---

## ğŸ“š Key Takeaways

âœ… **Architecture:** 4 layers (Simulation â†’ Data â†’ Learning â†’ Storage)

âœ… **Training:** Fixed 30k steps, no periodic evaluation, eval once at end

âœ… **Data:** HDF5 for recording â†’ Parquet for training (compression + speed)

âœ… **Critical:** Action recording bug fix, image normalization consistency

âœ… **Results:** Loss decreased (4.3â†’0.04) but 0% success (needs more data/training)

**This is a complete, working learning pipeline** - you can now understand every component and how they connect! ğŸ‰
